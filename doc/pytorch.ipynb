{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGSvision\n",
    "This is the PyTorch version, you can find a TensorFlow example [here](https://colab.research.google.com/github/ViCCo-Group/THINGSvision/blob/master/doc/tensorflow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ICWd-3iA671"
   },
   "source": [
    "### Install thingsvision and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0nVMt-M_KX_"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade thingsvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yq-bNySyBGO-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import thingsvision.vision as vision\n",
    "import numpy as np\n",
    "\n",
    "from thingsvision.model_class import Model\n",
    "from google.colab import drive\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and feature directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHSuNkaIAZw2"
   },
   "source": [
    "Specify both `path/to/images` (input directory) and `path/to/features` (output directory) on your Google Drive. \n",
    "The image directory is expected to contain images that are saved similarly to `/dog/img_1.png` or `/cat/img_1.jpg`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'path/to/images'  # path/to/images in GDrive\n",
    "output_dir = 'path/to/features' # path/to/output  in GDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF0R7sFu-7gI"
   },
   "source": [
    "Mount Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8nY_u1p-1F6",
    "outputId": "fd0e4a58-872c-4bfd-fb79-17586d6e48ce"
   },
   "outputs": [],
   "source": [
    "mounted_dir = '/thingsvision'\n",
    "drive.mount(mounted_dir, force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_path = os.path.join(mounted_dir, 'MyDrive', image_dir)\n",
    "full_output_path = os.path.join(mounted_dir, 'MyDrive', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "MkYIhI_P_Z6t",
    "outputId": "d1d1f8d3-16bc-43bf-f4ff-7b4a292e274f"
   },
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "                    model: Any,\n",
    "                    module_name: str,\n",
    "                    image_path: str,\n",
    "                    out_path: str,\n",
    "                    batch_size: int,\n",
    "                    flatten_activations: bool,\n",
    "                    apply_center_crop: bool,\n",
    "                    clip: bool=False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Extract features for a single layer.\"\"\"\n",
    "    dl = vision.load_dl(\n",
    "                        root=image_path,\n",
    "                        out_path=out_path,\n",
    "                        batch_size=batch_size,\n",
    "                        transforms=model.get_transformations(apply_center_crop=apply_center_crop),\n",
    "                        backend=backend,\n",
    "    )\n",
    "    # exctract features\n",
    "    features, _ = model.extract_features(\n",
    "                                        data_loader=dl,\n",
    "                                        module_name=module_name,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_acts=flatten_activations,\n",
    "                                        clip=clip,\n",
    "                                        return_probabilities=False,\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_all_layers(\n",
    "                        model_name: str,\n",
    "                        model: Any,\n",
    "                        image_path: str,\n",
    "                        out_path: str,\n",
    "                        batch_size: int,\n",
    "                        flatten_activations: bool,\n",
    "                        apply_center_crop: bool,\n",
    "                        layer: Any=nn.Linear,\n",
    "                        clip: bool=False,\n",
    ") -> None:\n",
    "    \"\"\"Extract features for all selected layers and save them to disk.\"\"\"\n",
    "    for module_name, module in model.model.named_modules():\n",
    "        if isinstance(module, layer):\n",
    "            # extract features for layer \"module_name\"\n",
    "            features = extract_features(\n",
    "                                        model=model,\n",
    "                                        module_name=module_name,\n",
    "                                        image_path=image_path,\n",
    "                                        out_path=out_path,\n",
    "                                        batch_size=batch_size,\n",
    "                                        flatten_activations=flatten_activations,\n",
    "                                        apply_center_crop=apply_center_crop,\n",
    "                                        clip=clip,\n",
    "            )\n",
    "            # save features to disk\n",
    "            vision.save_features(features, f'{out_path}/features_{model_name}_{module_name}', 'npy') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvJFUBiJ_j0W"
   },
   "outputs": [],
   "source": [
    "backend = 'pt' # backend 'pt' for PyTorch or 'tf' for Tensorflow \n",
    "pretrained = True # use pretrained model weights\n",
    "model_path = None # if pretrained = False (i.e., randomly initialized weights) set path to model weights\n",
    "batch_size = 32 # use a power of two (this can be any size, depending on the number of images for which you aim to extract features)\n",
    "apply_center_crop = True # center crop images (set to False, if you don't want to center-crop images)\n",
    "flatten_activations = True # whether or not features (e.g., of Conv layers) should be flattened\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `model` and `layer` for which you want to extract image features. If you want to extract features from a `torchvision` model, use the model naming defined [here](https://pytorch.org/vision/stable/models.html) (e.g., `vgg16` if you want to use VGG-16). If you are uncertain about the naming and enumeration of the layers, use `model.show()` to see how specific layers called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: VGG-16 with batch norm (pretrained on ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "model_name = 'vgg16_bn' \n",
    "model = Model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            backend=backend,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming\n",
    "# module_name = model.show() \n",
    "module_name = 'features.23' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract features for a single layer\n",
    "features = extract_features(\n",
    "                            model=model,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            clip=False,\n",
    ")\n",
    "\n",
    "# save features to disk\n",
    "vision.save_features(features, f'{full_output_path}/features_{model_name}_{module_name}', 'npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction all convolutional or fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all convolutional layers (i.e., Conv2d) and save them to disk\n",
    "layer = nn.Conv2d\n",
    "extract_all_layers(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    image_path=full_image_path,\n",
    "                    out_path=full_output_path,\n",
    "                    batch_size=batch_size,\n",
    "                    flatten_activations=flatten_activations,\n",
    "                    apply_center_crop=apply_center_crop,\n",
    "                    layer=layer,\n",
    "                    clip=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all fully-connected layers (i.e., Linear) and save them to disk\n",
    "layer = nn.Linear\n",
    "extract_all_layers(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    image_path=full_image_path,\n",
    "                    out_path=full_output_path,\n",
    "                    batch_size=batch_size,\n",
    "                    flatten_activations=flatten_activations,\n",
    "                    apply_center_crop=apply_center_crop,\n",
    "                    layer=layer,\n",
    "                    clip=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: VGG-16 with batch norm (pretrained on Ecoset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## load model\n",
    "model_name = 'VGG16bn_ecoset'\n",
    "model = Model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            backend=backend,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming\n",
    "# module_name = model.show() \n",
    "module_name = 'features.23' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "features = extract_features(\n",
    "                            model=model,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            clip=False,\n",
    ")\n",
    "\n",
    "# save features to disk\n",
    "vision.save_features(features, f'{full_output_path}/features_{model_name}_{module_name}', 'npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction all convolutional or fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all convolutional layers (i.e., Conv2d) and save them to disk\n",
    "layer = nn.Conv2d\n",
    "extract_all_layers(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    image_path=full_image_path,\n",
    "                    out_path=full_output_path,\n",
    "                    batch_size=batch_size,\n",
    "                    flatten_activations=flatten_activations,\n",
    "                    apply_center_crop=apply_center_crop,\n",
    "                    layer=layer,\n",
    "                    clip=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features for all fully-connected layers (i.e., Linear) and save them to disk\n",
    "layer = nn.Linear\n",
    "extract_all_layers(\n",
    "                    model_name=model_name,\n",
    "                    model=model,\n",
    "                    image_path=full_image_path,\n",
    "                    out_path=full_output_path,\n",
    "                    batch_size=batch_size,\n",
    "                    flatten_activations=flatten_activations,\n",
    "                    apply_center_crop=apply_center_crop,\n",
    "                    layer=layer,\n",
    "                    clip=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: CLIP (multimodal pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_name = 'clip-ViT'\n",
    "model = Model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            model_path=model_path,\n",
    "            device=device,\n",
    "            backend=backend,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select layer\n",
    "\n",
    "# NOTE: uncomment the line below, if you are uncertain about layer naming\n",
    "# module_name = model.show()\n",
    "module_name = 'visual' # penultimate layer in CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "features = extract_features(\n",
    "                            model=model,\n",
    "                            module_name=module_name,\n",
    "                            image_path=full_image_path,\n",
    "                            out_path=full_output_path,\n",
    "                            batch_size=batch_size,\n",
    "                            flatten_activations=flatten_activations,\n",
    "                            apply_center_crop=apply_center_crop,\n",
    "                            clip=True,\n",
    ")\n",
    "\n",
    "# apply centering (not necessary, but may be desirable, depending on the analysis)\n",
    "features = vision.center_features(features)\n",
    "\n",
    "# save features to disk\n",
    "vision.save_features(features, f'{full_output_path}/features_{model_name}_{module_name}', 'npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representational Similarity Analysis (RSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute representational dissimilarity matrix\n",
    "rdm = vision.compute_rdm(features, method='correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rdm\n",
    "vision.plot_rdm(\n",
    "                out_path,\n",
    "                features,\n",
    "                method='correlation',\n",
    "                format='.png', # '.jpg'\n",
    "                colormap='cividis',\n",
    "                show_plot=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "THINGSVISION.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
